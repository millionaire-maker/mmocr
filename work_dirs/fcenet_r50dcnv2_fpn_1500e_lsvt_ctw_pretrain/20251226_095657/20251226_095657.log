2025/12/26 09:56:58 - mmengine - INFO - 
------------------------------------------------------------
System environment:
    sys.platform: linux
    Python: 3.8.20 (default, Oct  3 2024, 15:24:27) [GCC 11.2.0]
    CUDA available: True
    MUSA available: False
    numpy_random_seed: 540706250
    GPU 0: NVIDIA GeForce RTX 3090
    CUDA_HOME: /usr/local/cuda
    NVCC: Cuda compilation tools, release 11.1, V11.1.105
    GCC: gcc (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
    PyTorch: 1.10.2+cu111
    PyTorch compiling details: PyTorch built with:
  - GCC 7.3
  - C++ Version: 201402
  - Intel(R) Math Kernel Library Version 2020.0.0 Product Build 20191122 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.2.3 (Git Hash 7336ca9f055cf1bfa13efb658fe15dc9b41f0740)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.1
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86
  - CuDNN 8.0.5
  - Magma 2.5.2
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.1, CUDNN_VERSION=8.0.5, CXX_COMPILER=/opt/rh/devtoolset-7/root/usr/bin/c++, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -DEDGE_PROFILER_USE_KINETO -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-variable -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.10.2, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, 

    TorchVision: 0.11.3+cu111
    OpenCV: 4.12.0
    MMEngine: 0.10.7

Runtime environment:
    cudnn_benchmark: True
    mp_cfg: {'mp_start_method': 'fork', 'opencv_num_threads': 0}
    dist_cfg: {'backend': 'nccl'}
    seed: 540706250
    Distributed launcher: none
    Distributed training: False
    GPU number: 1
------------------------------------------------------------

2025/12/26 09:56:59 - mmengine - INFO - Config:
auto_scale_lr = dict(base_batch_size=12, enable=True)
custom_hooks = [
    dict(
        min_delta=0.001,
        monitor='icdar/hmean',
        patience=6,
        rule='greater',
        type='EarlyStoppingHook'),
]
default_hooks = dict(
    checkpoint=dict(
        interval=5,
        max_keep_ckpts=3,
        rule='greater',
        save_best='icdar/hmean',
        type='CheckpointHook'),
    logger=dict(interval=5, type='LoggerHook'),
    param_scheduler=dict(type='ParamSchedulerHook'),
    sampler_seed=dict(type='DistSamplerSeedHook'),
    sync_buffer=dict(type='SyncBuffersHook'),
    timer=dict(type='IterTimerHook'),
    visualization=dict(
        draw_gt=False,
        draw_pred=False,
        enable=False,
        interval=1,
        show=False,
        type='VisualizationHook'))
default_scope = 'mmocr'
env_cfg = dict(
    cudnn_benchmark=True,
    dist_cfg=dict(backend='nccl'),
    mp_cfg=dict(mp_start_method='fork', opencv_num_threads=0))
launcher = 'none'
load_from = None
log_level = 'INFO'
log_processor = dict(by_epoch=True, type='LogProcessor', window_size=10)
max_epochs = 60
model = dict(
    backbone=dict(
        dcn=dict(deform_groups=2, fallback_on_stride=False, type='DCNv2'),
        depth=50,
        frozen_stages=-1,
        init_cfg=dict(checkpoint='torchvision://resnet50', type='Pretrained'),
        norm_cfg=dict(requires_grad=True, type='BN'),
        norm_eval=True,
        num_stages=4,
        out_indices=(
            1,
            2,
            3,
        ),
        stage_with_dcn=(
            False,
            True,
            True,
            True,
        ),
        style='pytorch',
        type='mmdet.ResNet'),
    data_preprocessor=dict(
        bgr_to_rgb=True,
        mean=[
            123.675,
            116.28,
            103.53,
        ],
        pad_size_divisor=32,
        std=[
            58.395,
            57.12,
            57.375,
        ],
        type='TextDetDataPreprocessor'),
    det_head=dict(
        fourier_degree=5,
        in_channels=256,
        module_loss=dict(
            level_proportion_range=(
                (
                    0,
                    0.25,
                ),
                (
                    0.2,
                    0.65,
                ),
                (
                    0.55,
                    1.0,
                ),
            ),
            num_sample=50,
            type='FCEModuleLoss'),
        postprocessor=dict(
            alpha=1.0,
            beta=2.0,
            num_reconstr_points=50,
            scales=(
                8,
                16,
                32,
            ),
            score_thr=0.3,
            text_repr_type='poly',
            type='FCEPostprocessor'),
        type='FCEHead'),
    neck=dict(
        act_cfg=None,
        add_extra_convs='on_output',
        in_channels=[
            512,
            1024,
            2048,
        ],
        num_outs=3,
        out_channels=256,
        relu_before_extra_convs=True,
        type='mmdet.FPN'),
    type='FCENet')
optim_wrapper = dict(
    clip_grad=dict(max_norm=5, norm_type=2),
    loss_scale='dynamic',
    optimizer=dict(lr=0.001, momentum=0.9, type='SGD', weight_decay=0.0005),
    type='AmpOptimWrapper')
param_scheduler = [
    dict(begin=0, by_epoch=True, end=2, start_factor=0.1, type='LinearLR'),
    dict(
        begin=2,
        by_epoch=True,
        end=60,
        eta_min=1e-07,
        power=0.9,
        type='PolyLR'),
]
randomness = dict(seed=None)
resume = False
test_cfg = dict(type='TestLoop')
test_dataloader = dict(
    batch_size=1,
    dataset=dict(
        datasets=[
            dict(
                ann_file='instances_val.json',
                data_root='data/lsvt_mmocr',
                pipeline=None,
                test_mode=True,
                type='OCRDataset'),
            dict(
                ann_file='instances_val.json',
                data_root='data/ctw_mmocr',
                pipeline=None,
                test_mode=True,
                type='OCRDataset'),
        ],
        pipeline=[
            dict(
                color_type='color_ignore_orientation',
                type='LoadImageFromFile'),
            dict(keep_ratio=True, scale=(
                2260,
                2260,
            ), type='Resize'),
            dict(
                type='LoadOCRAnnotations',
                with_bbox=True,
                with_label=True,
                with_polygon=True),
            dict(
                meta_keys=(
                    'img_path',
                    'ori_shape',
                    'img_shape',
                    'scale_factor',
                ),
                type='PackTextDetInputs'),
        ],
        type='ConcatDataset'),
    num_workers=2,
    persistent_workers=True,
    pin_memory=True,
    sampler=dict(shuffle=False, type='DefaultSampler'))
test_evaluator = dict(
    dataset_prefixes=dict(ctw='data/ctw_mmocr', lsvt='data/lsvt_mmocr'),
    type='MultiDatasetHmeanIOUMetric')
test_list = [
    dict(
        ann_file='instances_val.json',
        data_root='data/lsvt_mmocr',
        pipeline=None,
        test_mode=True,
        type='OCRDataset'),
    dict(
        ann_file='instances_val.json',
        data_root='data/ctw_mmocr',
        pipeline=None,
        test_mode=True,
        type='OCRDataset'),
]
test_pipeline = [
    dict(color_type='color_ignore_orientation', type='LoadImageFromFile'),
    dict(keep_ratio=True, scale=(
        2260,
        2260,
    ), type='Resize'),
    dict(
        type='LoadOCRAnnotations',
        with_bbox=True,
        with_label=True,
        with_polygon=True),
    dict(
        meta_keys=(
            'img_path',
            'ori_shape',
            'img_shape',
            'scale_factor',
        ),
        type='PackTextDetInputs'),
]
textdet_ctw_data_root = 'data/ctw_mmocr'
textdet_ctw_test = dict(
    ann_file='instances_val.json',
    data_root='data/ctw_mmocr',
    pipeline=None,
    test_mode=True,
    type='OCRDataset')
textdet_ctw_train = dict(
    ann_file='instances_train.json',
    data_root='data/ctw_mmocr',
    filter_cfg=dict(filter_empty_gt=True, min_size=32),
    pipeline=None,
    type='OCRDataset')
textdet_lsvt_ctw_data_root = 'data/textdet_pretrain_lsvt_ctw'
textdet_lsvt_ctw_test = dict(
    ann_file='instances_val.json',
    data_root='data/textdet_pretrain_lsvt_ctw',
    pipeline=None,
    test_mode=True,
    type='OCRDataset')
textdet_lsvt_ctw_train = dict(
    ann_file='instances_train.json',
    data_root='data/textdet_pretrain_lsvt_ctw',
    filter_cfg=dict(filter_empty_gt=True, min_size=32),
    pipeline=None,
    type='OCRDataset')
textdet_lsvt_data_root = 'data/lsvt_mmocr'
textdet_lsvt_test = dict(
    ann_file='instances_val.json',
    data_root='data/lsvt_mmocr',
    pipeline=None,
    test_mode=True,
    type='OCRDataset')
textdet_lsvt_train = dict(
    ann_file='instances_train.json',
    data_root='data/lsvt_mmocr',
    filter_cfg=dict(filter_empty_gt=True, min_size=32),
    pipeline=None,
    type='OCRDataset')
train_cfg = dict(max_epochs=60, type='EpochBasedTrainLoop', val_interval=5)
train_dataloader = dict(
    batch_size=20,
    dataset=dict(
        datasets=[
            dict(
                ann_file='instances_train.json',
                data_root='data/lsvt_mmocr',
                filter_cfg=dict(filter_empty_gt=True, min_size=32),
                pipeline=None,
                type='OCRDataset'),
            dict(
                ann_file='instances_train.json',
                data_root='data/ctw_mmocr',
                filter_cfg=dict(filter_empty_gt=True, min_size=32),
                pipeline=None,
                type='OCRDataset'),
        ],
        pipeline=[
            dict(
                color_type='color_ignore_orientation',
                type='LoadImageFromFile'),
            dict(
                type='LoadOCRAnnotations',
                with_bbox=True,
                with_label=True,
                with_polygon=True),
            dict(
                keep_ratio=True,
                ratio_range=(
                    0.75,
                    2.5,
                ),
                scale=(
                    800,
                    800,
                ),
                type='RandomResize'),
            dict(
                crop_ratio=0.5,
                iter_num=1,
                min_area_ratio=0.2,
                type='TextDetRandomCropFlip'),
            dict(
                prob=0.8,
                transforms=[
                    dict(min_side_ratio=0.3, type='RandomCrop'),
                ],
                type='RandomApply'),
            dict(
                prob=0.5,
                transforms=[
                    dict(
                        max_angle=30,
                        pad_with_fixed_color=False,
                        type='RandomRotate',
                        use_canvas=True),
                ],
                type='RandomApply'),
            dict(
                prob=[
                    0.6,
                    0.4,
                ],
                transforms=[
                    [
                        dict(keep_ratio=True, scale=800, type='Resize'),
                        dict(target_scale=800, type='SourceImagePad'),
                    ],
                    dict(keep_ratio=False, scale=800, type='Resize'),
                ],
                type='RandomChoice'),
            dict(direction='horizontal', prob=0.5, type='RandomFlip'),
            dict(
                brightness=0.12549019607843137,
                contrast=0.5,
                op='ColorJitter',
                saturation=0.5,
                type='TorchVisionWrapper'),
            dict(
                meta_keys=(
                    'img_path',
                    'ori_shape',
                    'img_shape',
                    'scale_factor',
                ),
                type='PackTextDetInputs'),
        ],
        type='ConcatDataset'),
    num_workers=6,
    persistent_workers=True,
    pin_memory=True,
    sampler=dict(shuffle=True, type='DefaultSampler'))
train_list = [
    dict(
        ann_file='instances_train.json',
        data_root='data/lsvt_mmocr',
        filter_cfg=dict(filter_empty_gt=True, min_size=32),
        pipeline=None,
        type='OCRDataset'),
    dict(
        ann_file='instances_train.json',
        data_root='data/ctw_mmocr',
        filter_cfg=dict(filter_empty_gt=True, min_size=32),
        pipeline=None,
        type='OCRDataset'),
]
train_pipeline = [
    dict(color_type='color_ignore_orientation', type='LoadImageFromFile'),
    dict(
        type='LoadOCRAnnotations',
        with_bbox=True,
        with_label=True,
        with_polygon=True),
    dict(
        keep_ratio=True,
        ratio_range=(
            0.75,
            2.5,
        ),
        scale=(
            800,
            800,
        ),
        type='RandomResize'),
    dict(
        crop_ratio=0.5,
        iter_num=1,
        min_area_ratio=0.2,
        type='TextDetRandomCropFlip'),
    dict(
        prob=0.8,
        transforms=[
            dict(min_side_ratio=0.3, type='RandomCrop'),
        ],
        type='RandomApply'),
    dict(
        prob=0.5,
        transforms=[
            dict(
                max_angle=30,
                pad_with_fixed_color=False,
                type='RandomRotate',
                use_canvas=True),
        ],
        type='RandomApply'),
    dict(
        prob=[
            0.6,
            0.4,
        ],
        transforms=[
            [
                dict(keep_ratio=True, scale=800, type='Resize'),
                dict(target_scale=800, type='SourceImagePad'),
            ],
            dict(keep_ratio=False, scale=800, type='Resize'),
        ],
        type='RandomChoice'),
    dict(direction='horizontal', prob=0.5, type='RandomFlip'),
    dict(
        brightness=0.12549019607843137,
        contrast=0.5,
        op='ColorJitter',
        saturation=0.5,
        type='TorchVisionWrapper'),
    dict(
        meta_keys=(
            'img_path',
            'ori_shape',
            'img_shape',
            'scale_factor',
        ),
        type='PackTextDetInputs'),
]
val_cfg = dict(type='ValLoop')
val_dataloader = dict(
    batch_size=1,
    dataset=dict(
        datasets=[
            dict(
                ann_file='instances_val.json',
                data_root='data/lsvt_mmocr',
                pipeline=None,
                test_mode=True,
                type='OCRDataset'),
            dict(
                ann_file='instances_val.json',
                data_root='data/ctw_mmocr',
                pipeline=None,
                test_mode=True,
                type='OCRDataset'),
        ],
        pipeline=[
            dict(
                color_type='color_ignore_orientation',
                type='LoadImageFromFile'),
            dict(keep_ratio=True, scale=(
                2260,
                2260,
            ), type='Resize'),
            dict(
                type='LoadOCRAnnotations',
                with_bbox=True,
                with_label=True,
                with_polygon=True),
            dict(
                meta_keys=(
                    'img_path',
                    'ori_shape',
                    'img_shape',
                    'scale_factor',
                ),
                type='PackTextDetInputs'),
        ],
        type='ConcatDataset'),
    num_workers=2,
    persistent_workers=True,
    pin_memory=True,
    sampler=dict(shuffle=False, type='DefaultSampler'))
val_evaluator = dict(
    dataset_prefixes=dict(ctw='data/ctw_mmocr', lsvt='data/lsvt_mmocr'),
    type='MultiDatasetHmeanIOUMetric')
vis_backends = [
    dict(type='LocalVisBackend'),
]
visualizer = dict(
    name='visualizer',
    type='TextDetLocalVisualizer',
    vis_backends=[
        dict(type='LocalVisBackend'),
    ])
work_dir = 'work_dirs/fcenet_r50dcnv2_fpn_1500e_lsvt_ctw_pretrain'

2025/12/26 09:57:06 - mmengine - INFO - Distributed training is not used, all SyncBatchNorm (SyncBN) layers in the model will be automatically reverted to BatchNormXd layers if they are used.
2025/12/26 09:57:06 - mmengine - INFO - Hooks will be executed in the following order:
before_run:
(VERY_HIGH   ) RuntimeInfoHook                    
(BELOW_NORMAL) LoggerHook                         
(LOWEST      ) EarlyStoppingHook                  
 -------------------- 
before_train:
(VERY_HIGH   ) RuntimeInfoHook                    
(NORMAL      ) IterTimerHook                      
(VERY_LOW    ) CheckpointHook                     
 -------------------- 
before_train_epoch:
(VERY_HIGH   ) RuntimeInfoHook                    
(NORMAL      ) IterTimerHook                      
(NORMAL      ) DistSamplerSeedHook                
 -------------------- 
before_train_iter:
(VERY_HIGH   ) RuntimeInfoHook                    
(NORMAL      ) IterTimerHook                      
 -------------------- 
after_train_iter:
(VERY_HIGH   ) RuntimeInfoHook                    
(NORMAL      ) IterTimerHook                      
(BELOW_NORMAL) LoggerHook                         
(LOW         ) ParamSchedulerHook                 
(VERY_LOW    ) CheckpointHook                     
 -------------------- 
after_train_epoch:
(NORMAL      ) IterTimerHook                      
(NORMAL      ) SyncBuffersHook                    
(LOW         ) ParamSchedulerHook                 
(VERY_LOW    ) CheckpointHook                     
 -------------------- 
before_val:
(VERY_HIGH   ) RuntimeInfoHook                    
 -------------------- 
before_val_epoch:
(NORMAL      ) IterTimerHook                      
(NORMAL      ) SyncBuffersHook                    
 -------------------- 
before_val_iter:
(NORMAL      ) IterTimerHook                      
 -------------------- 
after_val_iter:
(NORMAL      ) IterTimerHook                      
(NORMAL      ) VisualizationHook                  
(BELOW_NORMAL) LoggerHook                         
 -------------------- 
after_val_epoch:
(VERY_HIGH   ) RuntimeInfoHook                    
(NORMAL      ) IterTimerHook                      
(BELOW_NORMAL) LoggerHook                         
(LOW         ) ParamSchedulerHook                 
(VERY_LOW    ) CheckpointHook                     
(LOWEST      ) EarlyStoppingHook                  
 -------------------- 
after_val:
(VERY_HIGH   ) RuntimeInfoHook                    
 -------------------- 
after_train:
(VERY_HIGH   ) RuntimeInfoHook                    
(VERY_LOW    ) CheckpointHook                     
 -------------------- 
before_test:
(VERY_HIGH   ) RuntimeInfoHook                    
 -------------------- 
before_test_epoch:
(NORMAL      ) IterTimerHook                      
 -------------------- 
before_test_iter:
(NORMAL      ) IterTimerHook                      
 -------------------- 
after_test_iter:
(NORMAL      ) IterTimerHook                      
(NORMAL      ) VisualizationHook                  
(BELOW_NORMAL) LoggerHook                         
 -------------------- 
after_test_epoch:
(VERY_HIGH   ) RuntimeInfoHook                    
(NORMAL      ) IterTimerHook                      
(BELOW_NORMAL) LoggerHook                         
 -------------------- 
after_test:
(VERY_HIGH   ) RuntimeInfoHook                    
 -------------------- 
after_run:
(BELOW_NORMAL) LoggerHook                         
 -------------------- 
2025/12/26 09:57:24 - mmengine - INFO - LR is set based on batch size of 12 and the current batch size is 20. Scaling the original LR by 1.6666666666666667.
2025/12/26 09:57:27 - mmengine - INFO - load model from: torchvision://resnet50
2025/12/26 09:57:27 - mmengine - INFO - Loads checkpoint by torchvision backend from path: torchvision://resnet50
2025/12/26 09:57:27 - mmengine - WARNING - The model and loaded state dict do not match exactly

unexpected key in source state_dict: fc.weight, fc.bias

missing keys in source state_dict: layer2.0.conv2.conv_offset.weight, layer2.0.conv2.conv_offset.bias, layer2.1.conv2.conv_offset.weight, layer2.1.conv2.conv_offset.bias, layer2.2.conv2.conv_offset.weight, layer2.2.conv2.conv_offset.bias, layer2.3.conv2.conv_offset.weight, layer2.3.conv2.conv_offset.bias, layer3.0.conv2.conv_offset.weight, layer3.0.conv2.conv_offset.bias, layer3.1.conv2.conv_offset.weight, layer3.1.conv2.conv_offset.bias, layer3.2.conv2.conv_offset.weight, layer3.2.conv2.conv_offset.bias, layer3.3.conv2.conv_offset.weight, layer3.3.conv2.conv_offset.bias, layer3.4.conv2.conv_offset.weight, layer3.4.conv2.conv_offset.bias, layer3.5.conv2.conv_offset.weight, layer3.5.conv2.conv_offset.bias, layer4.0.conv2.conv_offset.weight, layer4.0.conv2.conv_offset.bias, layer4.1.conv2.conv_offset.weight, layer4.1.conv2.conv_offset.bias, layer4.2.conv2.conv_offset.weight, layer4.2.conv2.conv_offset.bias

Name of parameter - Initialization information

backbone.conv1.weight - torch.Size([64, 3, 7, 7]): 
PretrainedInit: load from torchvision://resnet50 

backbone.bn1.weight - torch.Size([64]): 
PretrainedInit: load from torchvision://resnet50 

backbone.bn1.bias - torch.Size([64]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.0.conv1.weight - torch.Size([64, 64, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.0.bn1.weight - torch.Size([64]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.0.bn1.bias - torch.Size([64]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.0.conv2.weight - torch.Size([64, 64, 3, 3]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.0.bn2.weight - torch.Size([64]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.0.bn2.bias - torch.Size([64]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.0.conv3.weight - torch.Size([256, 64, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.0.bn3.weight - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.0.bn3.bias - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.0.downsample.0.weight - torch.Size([256, 64, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.0.downsample.1.weight - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.0.downsample.1.bias - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.1.conv1.weight - torch.Size([64, 256, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.1.bn1.weight - torch.Size([64]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.1.bn1.bias - torch.Size([64]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.1.conv2.weight - torch.Size([64, 64, 3, 3]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.1.bn2.weight - torch.Size([64]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.1.bn2.bias - torch.Size([64]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.1.conv3.weight - torch.Size([256, 64, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.1.bn3.weight - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.1.bn3.bias - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.2.conv1.weight - torch.Size([64, 256, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.2.bn1.weight - torch.Size([64]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.2.bn1.bias - torch.Size([64]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.2.conv2.weight - torch.Size([64, 64, 3, 3]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.2.bn2.weight - torch.Size([64]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.2.bn2.bias - torch.Size([64]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.2.conv3.weight - torch.Size([256, 64, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.2.bn3.weight - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.2.bn3.bias - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.0.conv1.weight - torch.Size([128, 256, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.0.bn1.weight - torch.Size([128]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.0.bn1.bias - torch.Size([128]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.0.conv2.weight - torch.Size([128, 128, 3, 3]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.0.conv2.conv_offset.weight - torch.Size([54, 128, 3, 3]): 
The value is the same before and after calling `init_weights` of FCENet  

backbone.layer2.0.conv2.conv_offset.bias - torch.Size([54]): 
The value is the same before and after calling `init_weights` of FCENet  

backbone.layer2.0.bn2.weight - torch.Size([128]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.0.bn2.bias - torch.Size([128]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.0.conv3.weight - torch.Size([512, 128, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.0.bn3.weight - torch.Size([512]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.0.bn3.bias - torch.Size([512]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.0.downsample.0.weight - torch.Size([512, 256, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.0.downsample.1.weight - torch.Size([512]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.0.downsample.1.bias - torch.Size([512]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.1.conv1.weight - torch.Size([128, 512, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.1.bn1.weight - torch.Size([128]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.1.bn1.bias - torch.Size([128]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.1.conv2.weight - torch.Size([128, 128, 3, 3]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.1.conv2.conv_offset.weight - torch.Size([54, 128, 3, 3]): 
The value is the same before and after calling `init_weights` of FCENet  

backbone.layer2.1.conv2.conv_offset.bias - torch.Size([54]): 
The value is the same before and after calling `init_weights` of FCENet  

backbone.layer2.1.bn2.weight - torch.Size([128]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.1.bn2.bias - torch.Size([128]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.1.conv3.weight - torch.Size([512, 128, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.1.bn3.weight - torch.Size([512]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.1.bn3.bias - torch.Size([512]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.2.conv1.weight - torch.Size([128, 512, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.2.bn1.weight - torch.Size([128]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.2.bn1.bias - torch.Size([128]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.2.conv2.weight - torch.Size([128, 128, 3, 3]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.2.conv2.conv_offset.weight - torch.Size([54, 128, 3, 3]): 
The value is the same before and after calling `init_weights` of FCENet  

backbone.layer2.2.conv2.conv_offset.bias - torch.Size([54]): 
The value is the same before and after calling `init_weights` of FCENet  

backbone.layer2.2.bn2.weight - torch.Size([128]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.2.bn2.bias - torch.Size([128]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.2.conv3.weight - torch.Size([512, 128, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.2.bn3.weight - torch.Size([512]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.2.bn3.bias - torch.Size([512]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.3.conv1.weight - torch.Size([128, 512, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.3.bn1.weight - torch.Size([128]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.3.bn1.bias - torch.Size([128]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.3.conv2.weight - torch.Size([128, 128, 3, 3]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.3.conv2.conv_offset.weight - torch.Size([54, 128, 3, 3]): 
The value is the same before and after calling `init_weights` of FCENet  

backbone.layer2.3.conv2.conv_offset.bias - torch.Size([54]): 
The value is the same before and after calling `init_weights` of FCENet  

backbone.layer2.3.bn2.weight - torch.Size([128]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.3.bn2.bias - torch.Size([128]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.3.conv3.weight - torch.Size([512, 128, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.3.bn3.weight - torch.Size([512]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.3.bn3.bias - torch.Size([512]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.0.conv1.weight - torch.Size([256, 512, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.0.bn1.weight - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.0.bn1.bias - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.0.conv2.weight - torch.Size([256, 256, 3, 3]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.0.conv2.conv_offset.weight - torch.Size([54, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FCENet  

backbone.layer3.0.conv2.conv_offset.bias - torch.Size([54]): 
The value is the same before and after calling `init_weights` of FCENet  

backbone.layer3.0.bn2.weight - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.0.bn2.bias - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.0.conv3.weight - torch.Size([1024, 256, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.0.bn3.weight - torch.Size([1024]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.0.bn3.bias - torch.Size([1024]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.0.downsample.0.weight - torch.Size([1024, 512, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.0.downsample.1.weight - torch.Size([1024]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.0.downsample.1.bias - torch.Size([1024]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.1.conv1.weight - torch.Size([256, 1024, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.1.bn1.weight - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.1.bn1.bias - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.1.conv2.weight - torch.Size([256, 256, 3, 3]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.1.conv2.conv_offset.weight - torch.Size([54, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FCENet  

backbone.layer3.1.conv2.conv_offset.bias - torch.Size([54]): 
The value is the same before and after calling `init_weights` of FCENet  

backbone.layer3.1.bn2.weight - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.1.bn2.bias - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.1.conv3.weight - torch.Size([1024, 256, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.1.bn3.weight - torch.Size([1024]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.1.bn3.bias - torch.Size([1024]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.2.conv1.weight - torch.Size([256, 1024, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.2.bn1.weight - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.2.bn1.bias - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.2.conv2.weight - torch.Size([256, 256, 3, 3]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.2.conv2.conv_offset.weight - torch.Size([54, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FCENet  

backbone.layer3.2.conv2.conv_offset.bias - torch.Size([54]): 
The value is the same before and after calling `init_weights` of FCENet  

backbone.layer3.2.bn2.weight - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.2.bn2.bias - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.2.conv3.weight - torch.Size([1024, 256, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.2.bn3.weight - torch.Size([1024]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.2.bn3.bias - torch.Size([1024]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.3.conv1.weight - torch.Size([256, 1024, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.3.bn1.weight - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.3.bn1.bias - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.3.conv2.weight - torch.Size([256, 256, 3, 3]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.3.conv2.conv_offset.weight - torch.Size([54, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FCENet  

backbone.layer3.3.conv2.conv_offset.bias - torch.Size([54]): 
The value is the same before and after calling `init_weights` of FCENet  

backbone.layer3.3.bn2.weight - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.3.bn2.bias - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.3.conv3.weight - torch.Size([1024, 256, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.3.bn3.weight - torch.Size([1024]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.3.bn3.bias - torch.Size([1024]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.4.conv1.weight - torch.Size([256, 1024, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.4.bn1.weight - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.4.bn1.bias - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.4.conv2.weight - torch.Size([256, 256, 3, 3]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.4.conv2.conv_offset.weight - torch.Size([54, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FCENet  

backbone.layer3.4.conv2.conv_offset.bias - torch.Size([54]): 
The value is the same before and after calling `init_weights` of FCENet  

backbone.layer3.4.bn2.weight - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.4.bn2.bias - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.4.conv3.weight - torch.Size([1024, 256, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.4.bn3.weight - torch.Size([1024]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.4.bn3.bias - torch.Size([1024]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.5.conv1.weight - torch.Size([256, 1024, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.5.bn1.weight - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.5.bn1.bias - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.5.conv2.weight - torch.Size([256, 256, 3, 3]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.5.conv2.conv_offset.weight - torch.Size([54, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FCENet  

backbone.layer3.5.conv2.conv_offset.bias - torch.Size([54]): 
The value is the same before and after calling `init_weights` of FCENet  

backbone.layer3.5.bn2.weight - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.5.bn2.bias - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.5.conv3.weight - torch.Size([1024, 256, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.5.bn3.weight - torch.Size([1024]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.5.bn3.bias - torch.Size([1024]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.0.conv1.weight - torch.Size([512, 1024, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.0.bn1.weight - torch.Size([512]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.0.bn1.bias - torch.Size([512]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.0.conv2.weight - torch.Size([512, 512, 3, 3]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.0.conv2.conv_offset.weight - torch.Size([54, 512, 3, 3]): 
The value is the same before and after calling `init_weights` of FCENet  

backbone.layer4.0.conv2.conv_offset.bias - torch.Size([54]): 
The value is the same before and after calling `init_weights` of FCENet  

backbone.layer4.0.bn2.weight - torch.Size([512]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.0.bn2.bias - torch.Size([512]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.0.conv3.weight - torch.Size([2048, 512, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.0.bn3.weight - torch.Size([2048]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.0.bn3.bias - torch.Size([2048]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.0.downsample.0.weight - torch.Size([2048, 1024, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.0.downsample.1.weight - torch.Size([2048]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.0.downsample.1.bias - torch.Size([2048]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.1.conv1.weight - torch.Size([512, 2048, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.1.bn1.weight - torch.Size([512]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.1.bn1.bias - torch.Size([512]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.1.conv2.weight - torch.Size([512, 512, 3, 3]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.1.conv2.conv_offset.weight - torch.Size([54, 512, 3, 3]): 
The value is the same before and after calling `init_weights` of FCENet  

backbone.layer4.1.conv2.conv_offset.bias - torch.Size([54]): 
The value is the same before and after calling `init_weights` of FCENet  

backbone.layer4.1.bn2.weight - torch.Size([512]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.1.bn2.bias - torch.Size([512]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.1.conv3.weight - torch.Size([2048, 512, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.1.bn3.weight - torch.Size([2048]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.1.bn3.bias - torch.Size([2048]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.2.conv1.weight - torch.Size([512, 2048, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.2.bn1.weight - torch.Size([512]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.2.bn1.bias - torch.Size([512]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.2.conv2.weight - torch.Size([512, 512, 3, 3]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.2.conv2.conv_offset.weight - torch.Size([54, 512, 3, 3]): 
The value is the same before and after calling `init_weights` of FCENet  

backbone.layer4.2.conv2.conv_offset.bias - torch.Size([54]): 
The value is the same before and after calling `init_weights` of FCENet  

backbone.layer4.2.bn2.weight - torch.Size([512]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.2.bn2.bias - torch.Size([512]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.2.conv3.weight - torch.Size([2048, 512, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.2.bn3.weight - torch.Size([2048]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.2.bn3.bias - torch.Size([2048]): 
PretrainedInit: load from torchvision://resnet50 

neck.lateral_convs.0.conv.weight - torch.Size([256, 512, 1, 1]): 
XavierInit: gain=1, distribution=uniform, bias=0 

neck.lateral_convs.0.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FCENet  

neck.lateral_convs.1.conv.weight - torch.Size([256, 1024, 1, 1]): 
XavierInit: gain=1, distribution=uniform, bias=0 

neck.lateral_convs.1.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FCENet  

neck.lateral_convs.2.conv.weight - torch.Size([256, 2048, 1, 1]): 
XavierInit: gain=1, distribution=uniform, bias=0 

neck.lateral_convs.2.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FCENet  

neck.fpn_convs.0.conv.weight - torch.Size([256, 256, 3, 3]): 
XavierInit: gain=1, distribution=uniform, bias=0 

neck.fpn_convs.0.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FCENet  

neck.fpn_convs.1.conv.weight - torch.Size([256, 256, 3, 3]): 
XavierInit: gain=1, distribution=uniform, bias=0 

neck.fpn_convs.1.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FCENet  

neck.fpn_convs.2.conv.weight - torch.Size([256, 256, 3, 3]): 
XavierInit: gain=1, distribution=uniform, bias=0 

neck.fpn_convs.2.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FCENet  

det_head.out_conv_cls.weight - torch.Size([4, 256, 3, 3]): 
NormalInit: mean=0, std=0.01, bias=0 

det_head.out_conv_cls.bias - torch.Size([4]): 
NormalInit: mean=0, std=0.01, bias=0 

det_head.out_conv_reg.weight - torch.Size([22, 256, 3, 3]): 
NormalInit: mean=0, std=0.01, bias=0 

det_head.out_conv_reg.bias - torch.Size([22]): 
NormalInit: mean=0, std=0.01, bias=0 
2025/12/26 09:57:27 - mmengine - WARNING - "FileClient" will be deprecated in future. Please use io functions in https://mmengine.readthedocs.io/en/latest/api/fileio.html#file-io
2025/12/26 09:57:27 - mmengine - WARNING - "HardDiskBackend" is the alias of "LocalBackend" and the former will be deprecated in future.
2025/12/26 09:57:27 - mmengine - INFO - Checkpoints will be saved to /root/lanyun-tmp/mmocr/work_dirs/fcenet_r50dcnv2_fpn_1500e_lsvt_ctw_pretrain.
2025/12/26 09:57:45 - mmengine - INFO - Epoch(train)  [1][   5/2470]  lr: 1.6667e-04  eta: 6 days, 3:43:04  time: 3.5884  data_time: 0.6903  memory: 15896  grad_norm: inf  loss: 22.9630  loss_text: 2.8928  loss_center: 3.9387  loss_reg_x: 9.9731  loss_reg_y: 6.1583
2025/12/26 09:57:57 - mmengine - INFO - Epoch(train)  [1][  10/2470]  lr: 1.6667e-04  eta: 5 days, 0:48:21  time: 2.9348  data_time: 0.3465  memory: 12501  grad_norm: inf  loss: 22.1434  loss_text: 2.7933  loss_center: 3.9391  loss_reg_x: 9.8487  loss_reg_y: 5.5623
2025/12/26 09:58:07 - mmengine - INFO - Epoch(train)  [1][  15/2470]  lr: 1.6667e-04  eta: 4 days, 12:25:45  time: 2.1571  data_time: 0.0022  memory: 12501  grad_norm: 42.5953  loss: 21.6945  loss_text: 2.5333  loss_center: 3.9281  loss_reg_x: 10.0445  loss_reg_y: 5.1886
2025/12/26 09:58:19 - mmengine - INFO - Epoch(train)  [1][  20/2470]  lr: 1.6667e-04  eta: 4 days, 10:42:33  time: 2.2502  data_time: 0.0020  memory: 12501  grad_norm: 34.3142  loss: 21.0108  loss_text: 2.2628  loss_center: 3.8734  loss_reg_x: 10.1403  loss_reg_y: 4.7343
2025/12/26 09:58:30 - mmengine - INFO - Epoch(train)  [1][  25/2470]  lr: 1.6667e-04  eta: 4 days, 6:35:07  time: 2.2797  data_time: 0.0020  memory: 12501  grad_norm: 30.4040  loss: 19.9341  loss_text: 2.1203  loss_center: 3.7853  loss_reg_x: 9.6039  loss_reg_y: 4.4246
2025/12/26 09:58:40 - mmengine - INFO - Epoch(train)  [1][  30/2470]  lr: 1.6667e-04  eta: 4 days, 3:31:43  time: 2.0696  data_time: 0.0020  memory: 12501  grad_norm: 31.0269  loss: 19.4370  loss_text: 2.0775  loss_center: 3.6997  loss_reg_x: 8.8125  loss_reg_y: 4.8473
2025/12/26 09:58:52 - mmengine - INFO - Epoch(train)  [1][  35/2470]  lr: 1.6667e-04  eta: 4 days, 3:07:15  time: 2.1984  data_time: 0.0020  memory: 12501  grad_norm: 30.4884  loss: 16.8407  loss_text: 1.9400  loss_center: 3.4201  loss_reg_x: 7.3690  loss_reg_y: 4.1117
2025/12/26 09:59:01 - mmengine - INFO - Epoch(train)  [1][  40/2470]  lr: 1.6667e-04  eta: 4 days, 0:32:40  time: 2.1288  data_time: 0.0019  memory: 12501  grad_norm: 27.7460  loss: 16.1899  loss_text: 1.9410  loss_center: 3.2643  loss_reg_x: 7.0018  loss_reg_y: 3.9828
2025/12/26 09:59:12 - mmengine - INFO - Epoch(train)  [1][  45/2470]  lr: 1.6667e-04  eta: 4 days, 0:06:41  time: 2.0800  data_time: 0.0019  memory: 12501  grad_norm: 26.6768  loss: 17.3413  loss_text: 2.0508  loss_center: 3.3035  loss_reg_x: 7.4628  loss_reg_y: 4.5242
2025/12/26 09:59:23 - mmengine - INFO - Epoch(train)  [1][  50/2470]  lr: 1.6667e-04  eta: 3 days, 22:54:34  time: 2.1480  data_time: 0.0020  memory: 12501  grad_norm: 28.9604  loss: 16.7012  loss_text: 2.0040  loss_center: 3.2285  loss_reg_x: 7.1820  loss_reg_y: 4.2868
2025/12/26 09:59:33 - mmengine - INFO - Epoch(train)  [1][  55/2470]  lr: 1.6667e-04  eta: 3 days, 22:10:14  time: 2.0769  data_time: 0.0020  memory: 12501  grad_norm: 28.5555  loss: 16.1346  loss_text: 1.9596  loss_center: 3.1864  loss_reg_x: 6.9062  loss_reg_y: 4.0824
2025/12/26 09:59:45 - mmengine - INFO - Epoch(train)  [1][  60/2470]  lr: 1.6667e-04  eta: 3 days, 22:26:10  time: 2.2382  data_time: 0.0020  memory: 12501  grad_norm: 25.1559  loss: 15.8370  loss_text: 1.9447  loss_center: 3.1680  loss_reg_x: 6.7211  loss_reg_y: 4.0032
2025/12/26 09:59:55 - mmengine - INFO - Epoch(train)  [1][  65/2470]  lr: 1.6667e-04  eta: 3 days, 21:30:20  time: 2.1843  data_time: 0.0020  memory: 12501  grad_norm: 22.0904  loss: 15.4871  loss_text: 1.8638  loss_center: 3.0891  loss_reg_x: 6.6811  loss_reg_y: 3.8532
2025/12/26 10:00:06 - mmengine - INFO - Epoch(train)  [1][  70/2470]  lr: 1.6667e-04  eta: 3 days, 21:06:48  time: 2.0710  data_time: 0.0022  memory: 12501  grad_norm: 21.3141  loss: 15.1819  loss_text: 1.7611  loss_center: 3.0160  loss_reg_x: 6.6451  loss_reg_y: 3.7597
2025/12/26 10:00:17 - mmengine - INFO - Epoch(train)  [1][  75/2470]  lr: 1.6667e-04  eta: 3 days, 21:00:27  time: 2.1827  data_time: 0.0022  memory: 12501  grad_norm: 22.1089  loss: 15.2111  loss_text: 1.7944  loss_center: 3.0827  loss_reg_x: 6.7175  loss_reg_y: 3.6166
2025/12/26 10:00:27 - mmengine - INFO - Epoch(train)  [1][  80/2470]  lr: 1.6667e-04  eta: 3 days, 20:29:55  time: 2.1447  data_time: 0.0019  memory: 12501  grad_norm: 21.9370  loss: 15.6846  loss_text: 1.8442  loss_center: 3.1500  loss_reg_x: 6.8381  loss_reg_y: 3.8522
2025/12/26 10:00:38 - mmengine - INFO - Epoch(train)  [1][  85/2470]  lr: 1.6667e-04  eta: 3 days, 20:13:09  time: 2.0989  data_time: 0.0021  memory: 12501  grad_norm: 21.1716  loss: 16.1528  loss_text: 1.8279  loss_center: 3.1478  loss_reg_x: 7.0910  loss_reg_y: 4.0861
2025/12/26 10:00:50 - mmengine - INFO - Epoch(train)  [1][  90/2470]  lr: 1.6667e-04  eta: 3 days, 20:43:46  time: 2.3000  data_time: 0.0022  memory: 12501  grad_norm: 19.9746  loss: 15.9188  loss_text: 1.8162  loss_center: 3.1452  loss_reg_x: 7.0953  loss_reg_y: 3.8621
2025/12/26 10:01:00 - mmengine - INFO - Epoch(train)  [1][  95/2470]  lr: 1.6667e-04  eta: 3 days, 20:10:43  time: 2.2335  data_time: 0.0019  memory: 12501  grad_norm: 24.3723  loss: 15.8724  loss_text: 1.8110  loss_center: 3.1433  loss_reg_x: 6.9659  loss_reg_y: 3.9522
2025/12/26 10:01:11 - mmengine - INFO - Epoch(train)  [1][ 100/2470]  lr: 1.6667e-04  eta: 3 days, 20:00:12  time: 2.0789  data_time: 0.0020  memory: 12501  grad_norm: 25.7922  loss: 15.4667  loss_text: 1.8079  loss_center: 3.1433  loss_reg_x: 6.7811  loss_reg_y: 3.7344
2025/12/26 10:01:21 - mmengine - INFO - Epoch(train)  [1][ 105/2470]  lr: 1.6667e-04  eta: 3 days, 19:42:56  time: 2.1240  data_time: 0.0023  memory: 12501  grad_norm: 20.8309  loss: 14.5619  loss_text: 1.7436  loss_center: 3.0722  loss_reg_x: 6.4481  loss_reg_y: 3.2981
2025/12/26 10:01:31 - mmengine - INFO - Epoch(train)  [1][ 110/2470]  lr: 1.6667e-04  eta: 3 days, 19:11:54  time: 2.0228  data_time: 0.0023  memory: 12501  grad_norm: 20.7668  loss: 15.4522  loss_text: 1.7313  loss_center: 3.0681  loss_reg_x: 6.8974  loss_reg_y: 3.7554
2025/12/26 10:01:42 - mmengine - INFO - Epoch(train)  [1][ 115/2470]  lr: 1.6667e-04  eta: 3 days, 19:11:30  time: 2.0848  data_time: 0.0021  memory: 12501  grad_norm: 25.3617  loss: 16.5210  loss_text: 1.7787  loss_center: 3.1337  loss_reg_x: 7.2754  loss_reg_y: 4.3332
2025/12/26 10:01:52 - mmengine - INFO - Epoch(train)  [1][ 120/2470]  lr: 1.6667e-04  eta: 3 days, 18:41:51  time: 2.0727  data_time: 0.0020  memory: 12501  grad_norm: 27.6447  loss: 16.0260  loss_text: 1.7802  loss_center: 3.1322  loss_reg_x: 7.0984  loss_reg_y: 4.0152
2025/12/26 10:02:05 - mmengine - INFO - Epoch(train)  [1][ 125/2470]  lr: 1.6667e-04  eta: 3 days, 19:13:53  time: 2.2308  data_time: 0.0020  memory: 12501  grad_norm: 24.8975  loss: 15.3555  loss_text: 1.7853  loss_center: 3.1339  loss_reg_x: 6.9108  loss_reg_y: 3.5255
2025/12/26 10:02:17 - mmengine - INFO - Epoch(train)  [1][ 130/2470]  lr: 1.6667e-04  eta: 3 days, 19:41:08  time: 2.5192  data_time: 0.0023  memory: 12501  grad_norm: 21.8423  loss: 15.3389  loss_text: 1.7680  loss_center: 3.1358  loss_reg_x: 6.8882  loss_reg_y: 3.5469
2025/12/26 10:02:27 - mmengine - INFO - Epoch(train)  [1][ 135/2470]  lr: 1.6667e-04  eta: 3 days, 19:14:06  time: 2.2212  data_time: 0.0024  memory: 12501  grad_norm: 22.6649  loss: 15.7908  loss_text: 1.7527  loss_center: 3.1355  loss_reg_x: 6.8755  loss_reg_y: 4.0271
2025/12/26 10:02:37 - mmengine - INFO - Epoch(train)  [1][ 140/2470]  lr: 1.6667e-04  eta: 3 days, 18:53:18  time: 1.9599  data_time: 0.0021  memory: 12501  grad_norm: 26.0739  loss: 15.7047  loss_text: 1.7653  loss_center: 3.1325  loss_reg_x: 6.7653  loss_reg_y: 4.0416
2025/12/26 10:02:48 - mmengine - INFO - Epoch(train)  [1][ 145/2470]  lr: 1.6667e-04  eta: 3 days, 19:00:34  time: 2.1409  data_time: 0.0021  memory: 12501  grad_norm: 25.4378  loss: 15.4874  loss_text: 1.7658  loss_center: 3.1311  loss_reg_x: 6.7956  loss_reg_y: 3.7949
2025/12/26 10:03:00 - mmengine - INFO - Epoch(train)  [1][ 150/2470]  lr: 1.6667e-04  eta: 3 days, 19:10:31  time: 2.3168  data_time: 0.0025  memory: 12501  grad_norm: 22.9932  loss: 15.1891  loss_text: 1.7649  loss_center: 3.1328  loss_reg_x: 6.6006  loss_reg_y: 3.6907
2025/12/26 10:03:11 - mmengine - INFO - Epoch(train)  [1][ 155/2470]  lr: 1.6667e-04  eta: 3 days, 19:09:27  time: 2.2711  data_time: 0.0024  memory: 12501  grad_norm: 22.1414  loss: 15.1619  loss_text: 1.7635  loss_center: 3.1352  loss_reg_x: 6.7230  loss_reg_y: 3.5402
2025/12/26 10:03:22 - mmengine - INFO - Epoch(train)  [1][ 160/2470]  lr: 1.6667e-04  eta: 3 days, 19:10:38  time: 2.2202  data_time: 0.0019  memory: 12501  grad_norm: 19.8182  loss: 14.7071  loss_text: 1.6942  loss_center: 3.0650  loss_reg_x: 6.6023  loss_reg_y: 3.3456
2025/12/26 10:03:33 - mmengine - INFO - Epoch(train)  [1][ 165/2470]  lr: 1.6667e-04  eta: 3 days, 19:00:50  time: 2.1615  data_time: 0.0021  memory: 12501  grad_norm: 21.7631  loss: 14.6367  loss_text: 1.6792  loss_center: 3.0630  loss_reg_x: 6.5349  loss_reg_y: 3.3595
2025/12/26 10:03:43 - mmengine - INFO - Epoch(train)  [1][ 170/2470]  lr: 1.6667e-04  eta: 3 days, 18:58:13  time: 2.1342  data_time: 0.0021  memory: 12501  grad_norm: 21.1178  loss: 15.1928  loss_text: 1.7317  loss_center: 3.1315  loss_reg_x: 7.0438  loss_reg_y: 3.2858
2025/12/26 10:03:54 - mmengine - INFO - Epoch(train)  [1][ 175/2470]  lr: 1.6667e-04  eta: 3 days, 18:57:22  time: 2.1913  data_time: 0.0021  memory: 12501  grad_norm: 20.0701  loss: 14.8576  loss_text: 1.7491  loss_center: 3.1295  loss_reg_x: 6.9174  loss_reg_y: 3.0616
2025/12/26 10:04:06 - mmengine - INFO - Epoch(train)  [1][ 180/2470]  lr: 1.6667e-04  eta: 3 days, 19:02:34  time: 2.2468  data_time: 0.0021  memory: 12501  grad_norm: 25.4195  loss: 15.0055  loss_text: 1.7559  loss_center: 3.1285  loss_reg_x: 6.6267  loss_reg_y: 3.4944
2025/12/26 10:04:17 - mmengine - INFO - Epoch(train)  [1][ 185/2470]  lr: 1.6667e-04  eta: 3 days, 19:00:47  time: 2.2404  data_time: 0.0020  memory: 12501  grad_norm: 24.8397  loss: 15.1792  loss_text: 1.7385  loss_center: 3.1291  loss_reg_x: 6.9434  loss_reg_y: 3.3682
2025/12/26 10:04:27 - mmengine - INFO - Epoch(train)  [1][ 190/2470]  lr: 1.6667e-04  eta: 3 days, 18:47:53  time: 2.1041  data_time: 0.0021  memory: 12501  grad_norm: 22.3530  loss: 15.6093  loss_text: 1.7253  loss_center: 3.1281  loss_reg_x: 7.3204  loss_reg_y: 3.4354
2025/12/26 10:04:37 - mmengine - INFO - Epoch(train)  [1][ 195/2470]  lr: 1.6667e-04  eta: 3 days, 18:38:17  time: 2.0387  data_time: 0.0020  memory: 12501  grad_norm: 26.1093  loss: 15.9498  loss_text: 1.7328  loss_center: 3.1270  loss_reg_x: 7.0312  loss_reg_y: 4.0587
2025/12/26 10:04:49 - mmengine - INFO - Epoch(train)  [1][ 200/2470]  lr: 1.6667e-04  eta: 3 days, 18:42:21  time: 2.1665  data_time: 0.0019  memory: 12501  grad_norm: 27.2406  loss: 15.6534  loss_text: 1.7377  loss_center: 3.1270  loss_reg_x: 6.9202  loss_reg_y: 3.8685
2025/12/26 10:05:00 - mmengine - INFO - Epoch(train)  [1][ 205/2470]  lr: 1.6667e-04  eta: 3 days, 18:44:14  time: 2.2571  data_time: 0.0019  memory: 12501  grad_norm: 24.4319  loss: 15.3658  loss_text: 1.7237  loss_center: 3.1278  loss_reg_x: 7.0126  loss_reg_y: 3.5016
2025/12/26 10:05:10 - mmengine - INFO - Epoch(train)  [1][ 210/2470]  lr: 1.6667e-04  eta: 3 days, 18:37:30  time: 2.1682  data_time: 0.0018  memory: 12501  grad_norm: 23.4368  loss: 15.0933  loss_text: 1.7160  loss_center: 3.1291  loss_reg_x: 6.8163  loss_reg_y: 3.4320
2025/12/26 10:05:21 - mmengine - INFO - Epoch(train)  [1][ 215/2470]  lr: 1.6667e-04  eta: 3 days, 18:34:48  time: 2.1282  data_time: 0.0018  memory: 12501  grad_norm: 24.7948  loss: 15.4734  loss_text: 1.7309  loss_center: 3.1285  loss_reg_x: 6.8833  loss_reg_y: 3.7307
2025/12/26 10:05:32 - mmengine - INFO - Epoch(train)  [1][ 220/2470]  lr: 1.6667e-04  eta: 3 days, 18:32:53  time: 2.1666  data_time: 0.0020  memory: 12501  grad_norm: inf  loss: 15.4981  loss_text: 1.7479  loss_center: 3.1275  loss_reg_x: 6.7937  loss_reg_y: 3.8290
2025/12/26 10:05:43 - mmengine - INFO - Epoch(train)  [1][ 225/2470]  lr: 1.6667e-04  eta: 3 days, 18:29:10  time: 2.1556  data_time: 0.0020  memory: 12501  grad_norm: inf  loss: 14.8342  loss_text: 1.7627  loss_center: 3.1290  loss_reg_x: 6.4176  loss_reg_y: 3.5250
2025/12/26 10:05:56 - mmengine - INFO - Epoch(train)  [1][ 230/2470]  lr: 1.6667e-04  eta: 3 days, 18:50:17  time: 2.3687  data_time: 0.0020  memory: 12501  grad_norm: 25.3428  loss: 15.0775  loss_text: 1.7565  loss_center: 3.1307  loss_reg_x: 6.6091  loss_reg_y: 3.5811
2025/12/26 10:06:05 - mmengine - INFO - Epoch(train)  [1][ 235/2470]  lr: 1.6667e-04  eta: 3 days, 18:31:17  time: 2.2251  data_time: 0.0020  memory: 12501  grad_norm: 25.9207  loss: 15.2678  loss_text: 1.7372  loss_center: 3.1296  loss_reg_x: 6.7808  loss_reg_y: 3.6201
2025/12/26 10:06:16 - mmengine - INFO - Epoch(train)  [1][ 240/2470]  lr: 1.6667e-04  eta: 3 days, 18:34:42  time: 2.0619  data_time: 0.0020  memory: 12501  grad_norm: 29.0902  loss: 15.2558  loss_text: 1.7101  loss_center: 3.1271  loss_reg_x: 6.8653  loss_reg_y: 3.5533
2025/12/26 10:06:26 - mmengine - INFO - Epoch(train)  [1][ 245/2470]  lr: 1.6667e-04  eta: 3 days, 18:25:44  time: 2.1508  data_time: 0.0020  memory: 12501  grad_norm: 28.3611  loss: 15.3036  loss_text: 1.6863  loss_center: 3.1266  loss_reg_x: 6.9277  loss_reg_y: 3.5630
2025/12/26 10:06:37 - mmengine - INFO - Epoch(train)  [1][ 250/2470]  lr: 1.6667e-04  eta: 3 days, 18:18:39  time: 2.0449  data_time: 0.0019  memory: 12501  grad_norm: 25.2281  loss: 15.1594  loss_text: 1.7067  loss_center: 3.1284  loss_reg_x: 6.7373  loss_reg_y: 3.5870
2025/12/26 10:06:48 - mmengine - INFO - Epoch(train)  [1][ 255/2470]  lr: 1.6667e-04  eta: 3 days, 18:22:31  time: 2.1709  data_time: 0.0018  memory: 12501  grad_norm: 29.6069  loss: 15.2862  loss_text: 1.7473  loss_center: 3.1303  loss_reg_x: 6.4018  loss_reg_y: 4.0068
2025/12/26 10:07:01 - mmengine - INFO - Epoch(train)  [1][ 260/2470]  lr: 1.6667e-04  eta: 3 days, 18:42:24  time: 2.4517  data_time: 0.0019  memory: 12501  grad_norm: 29.2967  loss: 14.9220  loss_text: 1.7441  loss_center: 3.1304  loss_reg_x: 6.4108  loss_reg_y: 3.6367
2025/12/26 10:07:13 - mmengine - INFO - Epoch(train)  [1][ 265/2470]  lr: 1.6667e-04  eta: 3 days, 18:50:44  time: 2.5063  data_time: 0.0020  memory: 12501  grad_norm: 30.4693  loss: 14.9137  loss_text: 1.7220  loss_center: 3.1289  loss_reg_x: 6.5829  loss_reg_y: 3.4800
2025/12/26 10:07:23 - mmengine - INFO - Epoch(train)  [1][ 270/2470]  lr: 1.6667e-04  eta: 3 days, 18:39:35  time: 2.1806  data_time: 0.0019  memory: 12501  grad_norm: 30.6997  loss: 15.6270  loss_text: 1.7107  loss_center: 3.1277  loss_reg_x: 6.7001  loss_reg_y: 4.0885
2025/12/26 10:07:36 - mmengine - INFO - Epoch(train)  [1][ 275/2470]  lr: 1.6667e-04  eta: 3 days, 18:55:53  time: 2.2723  data_time: 0.0019  memory: 12501  grad_norm: 30.0804  loss: 15.3879  loss_text: 1.7235  loss_center: 3.1268  loss_reg_x: 6.8276  loss_reg_y: 3.7101
2025/12/26 10:07:48 - mmengine - INFO - Epoch(train)  [1][ 280/2470]  lr: 1.6667e-04  eta: 3 days, 19:07:44  time: 2.5301  data_time: 0.0021  memory: 12501  grad_norm: 32.9666  loss: 14.8819  loss_text: 1.7376  loss_center: 3.1266  loss_reg_x: 6.6705  loss_reg_y: 3.3472
2025/12/26 10:08:00 - mmengine - INFO - Epoch(train)  [1][ 285/2470]  lr: 1.6667e-04  eta: 3 days, 19:14:36  time: 2.4336  data_time: 0.0020  memory: 12501  grad_norm: 33.2494  loss: 14.4316  loss_text: 1.6695  loss_center: 3.0576  loss_reg_x: 6.1736  loss_reg_y: 3.5309
2025/12/26 10:08:11 - mmengine - INFO - Epoch(train)  [1][ 290/2470]  lr: 1.6667e-04  eta: 3 days, 19:12:37  time: 2.2796  data_time: 0.0019  memory: 12501  grad_norm: 34.4294  loss: 14.5156  loss_text: 1.6709  loss_center: 3.0593  loss_reg_x: 6.0318  loss_reg_y: 3.7536
2025/12/26 10:08:22 - mmengine - INFO - Epoch(train)  [1][ 295/2470]  lr: 1.6667e-04  eta: 3 days, 19:08:22  time: 2.1504  data_time: 0.0020  memory: 12501  grad_norm: 33.4377  loss: 15.7360  loss_text: 1.7210  loss_center: 3.1288  loss_reg_x: 6.5899  loss_reg_y: 4.2963
2025/12/26 10:08:34 - mmengine - INFO - Epoch(train)  [1][ 300/2470]  lr: 1.6667e-04  eta: 3 days, 19:17:15  time: 2.2809  data_time: 0.0020  memory: 12501  grad_norm: 34.9374  loss: 15.5786  loss_text: 1.7190  loss_center: 3.1265  loss_reg_x: 6.5196  loss_reg_y: 4.2135
2025/12/26 10:08:46 - mmengine - INFO - Epoch(train)  [1][ 305/2470]  lr: 1.6667e-04  eta: 3 days, 19:21:20  time: 2.3835  data_time: 0.0020  memory: 12501  grad_norm: 33.2995  loss: 14.7513  loss_text: 1.7109  loss_center: 3.1263  loss_reg_x: 6.5053  loss_reg_y: 3.4087
2025/12/26 10:08:57 - mmengine - INFO - Epoch(train)  [1][ 310/2470]  lr: 1.6667e-04  eta: 3 days, 19:27:13  time: 2.3519  data_time: 0.0022  memory: 12501  grad_norm: 30.4374  loss: 14.9083  loss_text: 1.6708  loss_center: 3.1271  loss_reg_x: 6.4034  loss_reg_y: 3.7070
2025/12/26 10:09:07 - mmengine - INFO - Epoch(train)  [1][ 315/2470]  lr: 1.6667e-04  eta: 3 days, 19:15:01  time: 2.1477  data_time: 0.0022  memory: 12501  grad_norm: 32.9979  loss: 15.4227  loss_text: 1.6873  loss_center: 3.1271  loss_reg_x: 6.3776  loss_reg_y: 4.2308
2025/12/26 10:09:17 - mmengine - INFO - Epoch(train)  [1][ 320/2470]  lr: 1.6667e-04  eta: 3 days, 19:07:05  time: 1.9696  data_time: 0.0021  memory: 12501  grad_norm: 30.9306  loss: 15.1110  loss_text: 1.6992  loss_center: 3.1275  loss_reg_x: 6.9257  loss_reg_y: 3.3585
2025/12/26 10:09:28 - mmengine - INFO - Epoch(train)  [1][ 325/2470]  lr: 1.6667e-04  eta: 3 days, 19:04:29  time: 2.0874  data_time: 0.0022  memory: 12501  grad_norm: 27.3460  loss: 14.7628  loss_text: 1.7055  loss_center: 3.1271  loss_reg_x: 6.9810  loss_reg_y: 2.9491
2025/12/26 10:09:41 - mmengine - INFO - Epoch(train)  [1][ 330/2470]  lr: 1.6667e-04  eta: 3 days, 19:21:04  time: 2.4105  data_time: 0.0022  memory: 12501  grad_norm: 33.0801  loss: 15.0640  loss_text: 1.7118  loss_center: 3.1259  loss_reg_x: 6.6329  loss_reg_y: 3.5934
2025/12/26 10:09:55 - mmengine - INFO - Epoch(train)  [1][ 335/2470]  lr: 1.6667e-04  eta: 3 days, 19:41:45  time: 2.7289  data_time: 0.0022  memory: 12501  grad_norm: 33.5422  loss: 14.7264  loss_text: 1.6885  loss_center: 3.1265  loss_reg_x: 6.2884  loss_reg_y: 3.6230
2025/12/26 10:10:06 - mmengine - INFO - Epoch(train)  [1][ 340/2470]  lr: 1.6667e-04  eta: 3 days, 19:38:37  time: 2.4712  data_time: 0.0022  memory: 12501  grad_norm: 29.0769  loss: 14.4385  loss_text: 1.6824  loss_center: 3.1280  loss_reg_x: 6.5237  loss_reg_y: 3.1043
